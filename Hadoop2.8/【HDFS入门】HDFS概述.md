###**1.概念**

* HDFS（Hadoop Distributed File System）是Hadoop项目的核心子项目，是分布式计算中数据存储管理的基础，是基于流数据模式访问和处理超大文件的需求而开发的，可以运行于廉价的商用服务器上。

* HDFS是一个主/从（Mater/Slave）体系结构，从最终用户的角度来看，它就像传统的文件系统一样，可以通过目录路径对文件执行CRUD（Create、Read、Update和Delete）操作。但由于分布式存储的性质，

* Hadoop所具有的高容错、高可靠性、高可扩展性、高获得性、高吞吐率等特征为海量数据提供了不怕故障的存储，为超大数据集（Large Data Set）的应用处理带来了很多便利。


###**2.组成**

![这里写图片描述](http://img.blog.csdn.net/20170728104609306?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYmFpeWVfeGluZw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

分析：

* 从上图可以看出HDFS集群拥有一个NameNode和一些DataNode。它们以管理者-工作者模式运行。即一个namenode，多个datanode，

* NameNode管理文件系统的命名空间，维护文件系统树及整棵树内所有文件和目录，即命名空间镜像文件和编辑日志文件。

* SecondaryNameNode的任务
定期合并fsimage和editlog，并传输给NameNode。
为NameNode提供热备份。

* DataNode 是文件系统的工作节点，根据需要存储并检索数据块（客户端或NameNode调度），定期向NameNode发送它们所存储块的列表。

* Client通过同NameNode和DataNodes的交互访问文件系统。客户端联系NameNode以获取文件的元数据，而真正的文件I/O操作是直接和DataNode进行交互的。

* 对NameNode实现容错：备份组成文件系统元数据持久状态的文件，或者运行一个Secondary NameNode，定期编辑日志合并命名空间镜像，但会滞后于NameNode，损失一部分数据

* 数据块默认大小64MB、块较大的原因是为了最小化寻址开销；使用抽象块作为存储单元，简化了存储子系统的设计；块还适合于数据备份而提供容错能力和提高可用性。

* 为保证高可用性，可以增加一个热备份NameNode。

###**3.优点**

（1）大数据文件

非常适合上T级别的大文件或者一堆大数据文件的存储，如果文件只有几个G甚至更小就没啥意思了。

（2）文件分块存储

HDFS会将一个完整的大文件平均分块存储到不同计算器上，它的意义在于读取文件时可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高得多得都。

（3）流式数据访问：一次写入多次读写

这种模式跟传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化也只能在文件末添加内容。

（4）廉价硬件
HDFS可以应用在普通PC机上，这种机制能够让给一些公司用几十台廉价的计算机就可以撑起一个大数据集群。

（5）硬件故障
HDFS认为所有计算机都可能会出问题，为了防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其它某几个主机上，如果其中一台主机失效，可以迅速找另一块副本取文件。

小结：Hadoop可以流式访问大文件，部署在廉价的集群上。


###**4.缺点及改进策略**

（1）不适合低延迟数据访问

* 缺点
如果要处理一些用户要求时间比较短的低延迟应用请求，则HDFS不适合。HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的，这就可能要求以高延迟作为代价。

* 改进策略
对于那些有低延时要求的应用程序，HBase是一个更好的选择。通过上层数据管理项 目来尽可能地弥补这个不足。在性能上有了很大的提升，它的口号就是goes real time。使用缓存或多master设计可以降低client的数据请求压力，以减少延时。还有就是对HDFS系统内部的修改，这就得权衡大吞吐量与低延 时了，HDFS不是万能的银弹。

（2）无法高效存储大量小文件

* 缺点
因为Namenode把文件系统的元数据放置在内存中，所以文件系统所能容纳的文件数目是由Namenode的内存大小来决定。一般来说，每一个文件、文件夹和Block需 要占据150字节左右的空间，所以，如果你有100万个文件，每一个占据一个Block，你就至少需要300MB内存。当前来说，数百万的文件还是可行 的，当扩展到数十亿时，对于当前的硬件水平来说就没法实现了。还有一个问题就是，因为Map task的数量是由splits来决定的，所以用MR处理大量的小文件时，就会产生过多的Maptask，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M，那就会有10000个Maptasks，会有很大的线程开销；若每个split为100M，则只有100个Maptasks，每个Maptask将会有更多的事情做，而线程的管理开销也将减小很多。

* 改进策略：
 * 利用SequenceFile、MapFile、Har等方式归档小文件，这个方法的原理就是把小文件归档起来管理，HBase就是基于此的。对于这种方法，如果想找回原来的小文件内容，那就必须得知道与归档文件的映射关系。
 * 横向扩展，一个Hadoop集群能管理的小文件有限，那就把几个Hadoop集群拖在一个虚拟服务器后面，形成一个大的Hadoop集群。google也是这么干过的。
 * 多Master设计，这个作用显而易见了。正在研发中的GFS II也要改为分布式多Master设计，还支持Master的Failover，而且Block大小改为1M，有意要调优处理小文件啊。
 * 附带个Alibaba DFS的设计，也是多Master设计，它把Metadata的映射存储和管理分开了，由多个Metadata存储节点和一个查询Master节点组成。

（3）不支持多用户写入及任意修改文件

* 在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。目前HDFS还不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改。


###**5.联邦HDFS**

* 联邦HDFS允许系统通过添加NameNode实现扩展。

* 每个namenode维护一个命名空间卷，包括命名空间的源数据和所有数据块的数据块池，而之前的NameNode是在内存中保存文件系统中每个文件和每个数据块的引用关系。






<br>
<br>
本人才疏学浅，若有错，请指出，谢谢！ 
如果你有更好的建议，可以留言我们一起讨论，共同进步！ 
衷心的感谢您能耐心的读完本篇博文！


参考链接：[HDFS初探之旅](http://www.cnblogs.com/xia520pi/archive/2012/05/28/2520813.html)